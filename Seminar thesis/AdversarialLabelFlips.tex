\documentclass{article}

\usepackage{microtype}
\usepackage{graphicx}
\usepackage{booktabs} 
\usepackage{hyperref}

\newcommand{\theHalgorithm}{\arabic{algorithm}}
\usepackage{amsmath,amssymb,amsfonts,amsthm,mathtools}
\usepackage{algorithmic}
\usepackage{textcomp}
\usepackage{diagbox}
\usepackage{float, multirow}
\usepackage{tikz, pgfplots}
\usepackage{tikzsymbols}
\usetikzlibrary{spy}
\usepackage{subcaption}
\usepgfplotslibrary{groupplots}
\pgfplotsset{compat=newest}
\usepackage{blindtext}


\begin{document}

\begin{titlepage}
	\noindent\makebox[\textwidth][l]{\includegraphics{universitaet-innsbruck-logo-cmyk-farbe.pdf}}
	\vspace{3cm}
	\begin{center}
		{\Large Seminar thesis}
		\vspace{50pt}\\
		\textbf{\Huge Adversarial Label Flips (sexier title?)}
		\vspace{40pt}\\
		\textbf{\Large Matthias Dellago \& Maximilian Samsinger}\vspace{20pt}\\
		{\large\today}
		\vspace{120pt}
	\end{center}
\end{titlepage}
	
	\DeclarePairedDelimiter\abs{\lvert}{\rvert}%
	\DeclarePairedDelimiter\norm{\lVert}{\rVert}%
	\DeclarePairedDelimiter\ceil{\lceil}{\rceil}
	\DeclarePairedDelimiter\floor{\lfloor}{\rfloor}
	
\begin{abstract}
	sell it to sb looking for something good to read.
	only 4 sentences or so.
\end{abstract}

\section{Introduction}

What is the open question?
How do we solve it.
\\

ie. what is our contribution. (we show that these confusion matrix are surprisingly nonrandom)

\\
use forward references i.e. "we elaborate on this in sectrion 4".\\
Give an example for a confusion matrix straight away.
\paragraph{Existence of adversarial examples}
Demonstrated that attacking deep neural networks are susceptible to attacks \cite{Szegedy13}. They actually coined the term "adversarial examples".

\section{Background and related work}

move it after the results so that the reader, can first get the interesting stuff, and then get the background?

\subsection{Attacks}

\paragraph{Fast gradient sign method}
\cite{goodfellow2014explaining} developed the fast gradient sign method. They are the guys with the panda image.

\paragraph{Projected gradient descent}
The projected gradient descent, which is basically iterated FGSM, was first shown in \cite{madry2017towards}. Their experiments suggest that these attacks converge, i.e. they find a local maxima. This may require some restarts.

\paragraph{Carlini-Wagner attack}


\paragraph{Foolbox}
A Python library with lots of attacks \cite{rauber2017foolbox}. They include the attacks above.

\subsection{Neural networks (Necessary)}
Is this section necessary? It seems that whoever is interested in our results, easily already knows this.
\texttt{No, we can just mention neural networks in the introduction and specify our models in the experiments section}

First introduced in \cite{lecun1999object}. The authors of
\cite{krizhevsky2012imagenet} demonstrated the effectiveness of deep convolutional neural networks on ImageNet.

\paragraph{ResNets}
Paradigm shift in deep learning. In \cite{he2016deep} they developed Residual Networks to train very deep neural networks. We will probably use ResNet18. If we do, we probably also cite \cite{he2016identity} for the "pre-activation" optimization. This is just a better architecture obtained by having BatchNorm-ReLU-Weights blocks instead of Weights-BatchNorm-ReLU blocks.


\section{Methods}

Reference to our github.

\subsection{Datasets}
MNIST, Fashion MNIST, CIFAR-10

\subsection{}
We probably use \url{https://arxiv.org/pdf/1608.04644.pdf} Table 1 as a neural network for MNIST \& Fashion-MNIST.

\section{Experiments}


\section{Results}
pictures, pictures, and maybe a graph or two


\section{Discussion}

Symmetry of matrices -> maybe find a way to quantify symmetry?\\
-> NN can recognise "similarity"\\
\\

Attractor classes -> manage with an extra "noise"-class or so?\\


In Figures X, Y and Z one can observe that adversarial examples computed with large perturbation budgets $\epsilon$ are misclassified as "8", "TODO" and "frog" for MNIST, Fashion-MNIST and CIFAR-10 respectively. In order to shed light onto this phenomenon we generate and classify $10000$ white noise images sampled from a uniform distribution on the input domain. Figure A shows that these randomly generated images are also, most commonly, classified as "8", "TODO" and "frog" respectively. This result suggests that the neural networks in question have a default output for low probability images with respect to distribution of the input domain, which in turn affects adversarial examples computed with large perturbation budgets.

\section{Conclusion}
What was the main idea.

\section{Contribution Statement}

\bibliographystyle{unsrt}
\bibliography{literature}

\end{document}
