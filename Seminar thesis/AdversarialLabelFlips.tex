\documentclass{article}

\usepackage{microtype}
\usepackage{graphicx}
\usepackage{booktabs} 
\usepackage{hyperref}

\newcommand{\theHalgorithm}{\arabic{algorithm}}
\usepackage{amsmath,amssymb,amsfonts,amsthm,mathtools}
\usepackage{algorithmic}
\usepackage{textcomp}
\usepackage{diagbox}
\usepackage{float, multirow}
\usepackage{tikz, pgfplots}
\usepackage{tikzsymbols}
\usetikzlibrary{spy}
\usepackage{subcaption}
\usepgfplotslibrary{groupplots}
\pgfplotsset{compat=newest}
\usepackage{blindtext}


\begin{document}

\begin{titlepage}
	\noindent\makebox[\textwidth][l]{\includegraphics{universitaet-innsbruck-logo-cmyk-farbe.pdf}}
	\vspace{3cm}
	\begin{center}
		{\Large Seminar thesis}
		\vspace{50pt}\\
		\textbf{\Huge Adversarial Label Flips}
		\vspace{40pt}\\
		\textbf{\Large Matthias Dellago \& Maximilian Samsinger}\vspace{20pt}\\
		{\large\today}
		\vspace{120pt}
	\end{center}
\end{titlepage}
	
	\DeclarePairedDelimiter\abs{\lvert}{\rvert}%
	\DeclarePairedDelimiter\norm{\lVert}{\rVert}%
	\DeclarePairedDelimiter\ceil{\lceil}{\rceil}
	\DeclarePairedDelimiter\floor{\lfloor}{\rfloor}
	
\begin{abstract}
	\blindtext
\end{abstract}

\section{Introduction}
Example citation \cite{madry2017towards}.


\section{Background and related work}

\subsection{Attacks}
\paragraph{Existence of adversarial examples}
Demonstrated that attacking deep neural networks are susceptible to attacks \cite{Szegedy13}. They actually coined the term "adversarial examples".

\paragraph{Fast gradient sign method}
\cite{goodfellow2014explaining} developed the fast gradient sign method. They are the guys with the panda image.

\paragraph{Projected gradient descent}
The projected gradient descent, which is basically iterated FGSM, was first shown in \cite{madry2017towards}. Their experiments suggest that these attacks converge, i.e. they find a local maxima. This may require some restarts.

\paragraph{Foolbox}
A Python library with lots of attacks \cite{rauber2017foolbox}. They include the attacks above.

\subsection{Neural networks}
First introduced in \cite{lecun1999object}. The authors of
\cite{krizhevsky2012imagenet} demonstrated the effectiveness of deep convolutional neural networks on ImageNet.

\paragraph{ResNets}
Paradigm shift in deep learning. In \cite{he2016deep} they developed Residual Networks to train very deep neural networks. We will probably use ResNet18. If we do, we probably also cite \cite{he2016identity} for the "pre-activation" optimization. This is just a better architecture obtained by having BatchNorm-ReLU-Weights blocks instead of Weights-BatchNorm-ReLU blocks.


\section{Methods}
\subsection{Datasets}

\subsection{}

\bibliographystyle{unsrt}
\bibliography{literature}

\end{document}
