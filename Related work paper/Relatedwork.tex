\documentclass{article}

\usepackage{microtype}
\usepackage{graphicx}
\usepackage{booktabs} 
\usepackage{hyperref}

\newcommand{\theHalgorithm}{\arabic{algorithm}}
\usepackage{amsmath,amssymb,amsfonts,amsthm,mathtools}
\usepackage{algorithmic}
\usepackage{textcomp}
\usepackage{diagbox}
\usepackage{float, multirow}
\usepackage{tikz, pgfplots}
\usepackage{tikzsymbols}
\usetikzlibrary{spy}
\usepackage{subcaption}
\usepgfplotslibrary{groupplots}
\pgfplotsset{compat=newest}
\usepackage{blindtext}


\begin{document}

\begin{titlepage}
	\noindent\makebox[\textwidth][l]{\includegraphics{universitaet-innsbruck-logo-cmyk-farbe.pdf}}
	\vspace{3cm}
	\begin{center}
		{\Large Related work}
		\vspace{50pt}\\
		\textbf{\Huge Adversarial Label Flips}
		\vspace{40pt}\\
		\textbf{\Large Matthias Dellago \& Maximilian Samsinger}\vspace{20pt}\\
		{\large\today}
		\vspace{120pt}
	\end{center}
\end{titlepage}
	
	\DeclarePairedDelimiter\abs{\lvert}{\rvert}%
	\DeclarePairedDelimiter\norm{\lVert}{\rVert}%
	\DeclarePairedDelimiter\ceil{\lceil}{\rceil}
	\DeclarePairedDelimiter\floor{\lfloor}{\rfloor}

\section{Notation}
We denote neural network classifiers with ${f_\theta\colon\mathcal{X}\to\mathcal{Y}, x\mapsto y}$ with trainable parameter $\theta$, where $\mathcal{X}$ are a set of images with corresponding labels (classes) $\mathcal{Y}$. The parameter $\theta$ are optimized by minimizing a training objective ${(\theta, x, y) \mapsto J(\theta, x, y)}$ with respect to $\theta$.
	
\section{On adversarial attacks}
Deep neural networks have been shown to be vulnerable to tiny, maliciously crafted perturbations applied to otherwise benign inputs. These so-called "adversarial examples" were first introduced in \cite{Szegedy13}. Further research \cite{goodfellow2014explaining} showed that these adversarial examples generalize over multiple dataset and architectures. Even very inexpensive attacks like the Fast Gradient Sign Method (FGSM) \cite{goodfellow2014explaining} can be used to fool neural networks. FGSM requires white-box access to the targeted neural networks architecture and its weights. Adversarial examples are computed by performing a gradient ascent step with respect to the sign of the gradient
\begin{align*}
	\operatorname{FGSM}_\epsilon(x) = x + \epsilon\operatorname{sign}(\nabla_x J(\theta,x,y)) 
\end{align*}
given a step size $\epsilon$. FGSM is an $L^\infty$-bounded attack\footnote{While bounds with respect to an $L^p$ norm are commonly used in the machine learning literature, we are aware that they are "[...] neither necessary nor sufficient for perceptual similarity [...]" \cite{sharif2018suitability}.}, i.e. $\norm{x-\operatorname{FGSM}_\epsilon(x)}_\infty\le\epsilon$, meaning that each pixel value of a benign image $x$ may only be perturbed by up to $\epsilon$.
Stronger attacks can be computed by repeatedly applying $\operatorname{FGSM}$ with smaller step sizes. This type of attack is known as Projected Gradient Descent (PGD) and was first introduced \cite{madry2017towards}. Their experiments demonstrated the effectiveness of such attacks and showed that convergence is achieved after only a few hundred iterations. 
%Finally \cite{goodfellow2014explaining} and \cite{madry2017towards}  

 

\paragraph{Foolbox}
For our experiments we will use a suit of different attacks using the FoolBox library \cite{rauber2017foolbox}. 


\section{On neural networks}
\subsection{Neural networks}
First introduced in \cite{lecun1999object}. The authors of
\cite{krizhevsky2012imagenet} demonstrated the effectiveness of deep convolutional neural networks on ImageNet.

\paragraph{ResNets}
Paradigm shift in deep learning. In \cite{he2016deep} they developed Residual Networks to train very deep neural networks. We will probably use ResNet18. If we do, we probably also cite \cite{he2016identity} for the "pre-activation" optimization. This is just a better architecture obtained by having BatchNorm-ReLU-Weights blocks instead of Weights-BatchNorm-ReLU blocks.


\section{Methods}
\subsection{Datasets}
MNIST, Fashion MNIST, CIFAR-10

\subsection{}
We probably use \url{https://arxiv.org/pdf/1608.04644.pdf} Table 1 as a neural network for MNIST \& Fashion-MNIST.

\bibliographystyle{unsrt}
\bibliography{literature}

\end{document}
